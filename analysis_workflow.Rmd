---
title: "Reproducing cl_workflow.py in R"
output:
  html_document:
    df_print: paged
---

```{r load libraries}
library(stm) # runs the topic models
library(quanteda) # makes the dictionary and corpus
library(tidyverse) # makes nice output and stuff
library(tidytext) # does amazing things with text
#library(tidystm) 
library(SnowballC)
library(stringdist)
library(ggmap)
#library(e1071)
```

Load the Training Set:

```{r read data}
cl_full <- read_csv('data/cl_full.csv') %>% #import cl data # maintain original indices
  mutate(py_index = 1:129500,
         log_price = log(cleanRent),# make a logged price variable
         log_sqft = log(cleanSqft)
         )# make a log square foot variable
cl_data <- cl_full # make a copied df to use below (because we'll drop duplicates)

``` 

Add Census Info, Add Context Columns

```{r adding census info}
tracts <- read_csv('resources/WAtracts.csv') %>% select(-X1) #read dataframe of census tracts for WA
re_cols <- c("white","black", "aindian", "asian","pacisland", "other", "latinx") # save a vector of race and ethnicity categories
tracts <- bind_cols(tracts, setNames(tracts[re_cols]/tracts$total_RE, paste0(re_cols,"_proportion"))) %>% # make racial proportion columns
  mutate(pov_proportion = under_poverty/total_poverty, # poverty proportion variable
         log_income = log(income), # log income variable
         pop_thousands = total_RE/1000)# population in thousands
cl_data <- inner_join(cl_data, tracts, by='GEOID10') %>% # join cl_data and tract data
  drop_na(black_proportion, asian_proportion, latinx_proportion, pop_thousands, log_income, log_price) # drop na values in covariate cols

```


Clean Text
```{r text cleaning}
neighborhoods <- read_file('resources/seattle_stop_words.txt') # read stopword file
pattern <- paste0('\\b',str_replace_all(neighborhoods,'\n', '|'),'\\b\\s*') # make regex
# drop urls
cl_data <- cl_data %>% mutate(cleanText = str_to_lower(listingText), # convert to lower
                              cleanText = str_replace_all(cleanText, # clean urls 
                                                           '(http)?(www)?\\S*(\\.com|\\.net|\\.gov|\\.be|\\.org)\\S*', ''),
                              cleanText = str_replace_all(cleanText, pattern, ''), # remove neighborhood names
                              cleanText = str_replace_all(cleanText,"(\\w+)\\W|/(\\w+)","\\1 \\2"), # fix words connected by - or / or another non-word character so that they're separated as a space
                              cleanText = ifelse(str_length(cleanText)>3,cleanText,NA), # sets texts to NA if they're less than 3 characters (Because otherwise they throw off the text processor below)
                              listingText = str_replace_all(listingText, 'QR Code Link to This Post', '') # remove this phrase from the listing text
                                                           ) %>% 
  # this will make some rows NA, drop them
  drop_na(cleanText) 

```


I decided that there was some meaty content in the listing titles. So the following block merges the titles with the clean text and then cleans with same format used on the texts above. The new variable is called fullText.

```{r complete prep on text}
cl_data <- cl_data %>%
  mutate(fullText = paste(listingTitle, listingText, sep='\n\n'),
         fullText = str_to_lower(fullText), # convert to lower
         fullText = str_replace_all(fullText, # clean urls 
                                      '(http)?(www)?\\S*(\\.com|\\.net|\\.gov|\\.be|\\.org)\\S*', ''),
         fullText = str_replace_all(fullText, pattern, ''), # remove neighborhood names
         fullText = str_replace_all(fullText,"(\\w+)\\W+|/(\\w+)","\\1 \\2"), # fix words connected by - or / or another non-word character so that they're separated as a space, and remove punctuation
         fullText = ifelse(str_length(fullText)>3,fullText,NA), # sets texts to NA if they're less than 3 characters (Because otherwise they throw off the text processor below)
         listingText = str_replace_all(listingText, 'QR Code Link to This Post', ''))  %>%  # remove this phrase from the listing text 
  drop_na(fullText) # this will make some rows NA, drop them
```



Clean duplicates
I picked .08 after exploring sorted and topic modeled texts to see what threshhold seemed to be closest to what I would sort as duplicates. I adjusted to a greedier matching (previously I used .05) after I was unhappy with the results once I started the qualitative coding. I also decreased the model total cutoff below from 50 to 40. That means I only stop searching for dupes when fitting a WHOLE NEW model only came up with 40 possible dupes. I also increased the max model iterations to 50, which makes these models much more like what I'll run eventually.

That was when I was using simple Jaccard matching which I think is fast and quite good for finding very close duplicates, say those as a result of different spellings. However, in my corpus there are documents which are made with a template by property manageers. These have more difference than a mispelling or two, and so single-character Jaccard matching has trouble distinguishing them from texts which just happen to be similar, we'll call  those 'false-dupes'. The false dupes problem meant that I was dropping thousands of texts from my corpus that weren't actually duplicates. To deal with that, I experimented with other string distance measured and eventually settled on Jaccard matching using 10-grams. That's sigificantly slower but also, based on my testing, seems to be more accurate: only very very low matches are false matches. In fact, the only true false-dupes I read were only around a .2 match (so they had a .8 measured jaccard 10-gram difference), compared to a .95 match for simple jaccard. Documents at around .7 difference were often from the same development or property manager but had different unit descriptions. Since those kinds of texts would be acceptable to include, I set the new threshold at .7 for jaccard 10-gram distance.


```{r set vars for dupe cleaning}
#start up vars
thresh = .7
interval = 99
n_topics = 40
cl_dropped <- cl_data
# cl_save <- cl_dropped
# cl_dropped <- cl_save
# cl_dropped_08 <- cl_dropped
```

I initially just dropped duplicates based on textual similarity using the LDA, then adjusted to use STM, but I found that I still had lots of very similar texts from the same addresses in my models. This means that my associations with racial proportion are more about the material differences in housing stock available in black neighborhoods (controlling for economic factors), then about discursive differences. The following routine robustly removes matches for each address in the sample. There are some issues: I'm not using fuzzy address matching, so some duplicates might come from the address being written multiple ways. Also, many times property management companies use similar texts across the properties they manange: those won't be sorted out by this method. However, by completing this step first, I make it easier for the next STM dupe removal step.

```{r drop by shared address}
# loop through addresses
addresses <- cl_dropped%>% group_by(matchAddress) %>% count() %>% arrange(desc(n)) %>% filter(n>10)
address_total <- 0
for(i in addresses$matchAddress){
  # grab the top 100 documents matching each topic
  top_100 <- cl_dropped %>% filter(matchAddress == i)
  # get the texts of those documents and compare them
  dist_matrix <- stringdistmatrix(top_100$cleanText, method = 'jaccard', q = 10)
  candidates <- as.data.frame(which(as.matrix(dist_matrix)<thresh, arr.ind = T)) %>% 
    filter(row!=col) %>% 
    mutate(candidate = ifelse(row>col,row,col))
  # get the ids of the ones to drop
  drop_list <- top_100$py_index[c(unique(candidates$candidate))]
  # drop them
  cl_dropped <- cl_dropped %>% filter(!py_index %in% drop_list)
  print(paste("Dropped", length(drop_list), "listings from",i)) # print for logging
  # update the total for this iteration
  address_total <- address_total + length(drop_list)
}
print(address_total)
```


```{r stm loop cleaning}
# loopy loops
# init a high model total
model_total=51
# loop while the model total is high
while(model_total>5){
  model_total=0 # reset model total
  # preprocess the documents, make a corpus
  temp <- textProcessor(documents = cl_dropped$cleanText, meta=cl_dropped, onlycharacter = TRUE) 
  out_dupe <- prepDocuments(temp$documents, temp$vocab, meta = temp$meta)
  # fit a limited model to that data
  dupe_model <- stm(out_dupe$documents, out_dupe$vocab, K = n_topics, max.em.its = 100, seed=24 ,verbose = TRUE)
  dupe_fit <- as_data_frame(make.dt(dupe_model, out_dupe$meta))
  # start some counters
  it_start = 1 
  it_total = 701
  # loop through iterations
  while(it_total>10){
    it_total = 0
    it_max = 0
    # loop through topics
    for(i in paste0('Topic',1:n_topics)){
      # grab the top 100 documents matching each topic
      top_100 <- dupe_fit %>% 
        arrange(get(i)) %>% 
        select(py_index) %>% 
        slice(it_start:(it_start+interval)) %>% 
        inner_join(cl_dropped, by='py_index') %>%
        select(py_index,cleanText)
      # get the texts of those documents and compare them
      dist_matrix <- stringdistmatrix(top_100$cleanText, method = 'jaccard', q = 10)
      candidates <- as.data.frame(which(as.matrix(dist_matrix)<thresh, arr.ind = T)) %>% 
        filter(row!=col) %>% 
        mutate(candidate = ifelse(row>col,row,col))
      # get the ids of the ones to drop
      drop_list <- top_100$py_index[c(unique(candidates$candidate))]
      # drop them
      cl_dropped <- cl_dropped %>% filter(!py_index %in% drop_list)
      # drop them from the fit object too
      dupe_fit <- dupe_fit %>% filter(!py_index %in% drop_list)
      print(paste("Dropped", length(drop_list), "listings from",i)) # print for logging
      # update the total for this iteration
      it_total <- it_total + length(drop_list)
      # if this topic beat the last max for this iteration update it
      if(length(drop_list)>it_max){
        it_max = length(drop_list)
      }
    }
    # update the start for this iteration
    it_start = it_start + 100 - it_max
    print(paste("Dropped",it_total,"total listings this iteration")) # print for logging
    model_total <- model_total + it_total
  }
  print(paste("Dropped",model_total,"total listings with this model")) # print for logging
}
```

notes for finding the best threshold:
8/7: .08 was way too greedy. I'm now looking at more like a .03 as a reasonable cutoff. I got non-matches even at .05. However, I haven't yet seen any non-matches under .04, and I've seen some very similar (but not exactly the same) matches at 0.039. I then observed some candidates at .047 and 0.48 which were clearly not matches: post id: 6470137879 and post id: 6537495772. Looking deeper, I found a match at 0.04347826 with post id: 6580790444 and post id: 6580676411

post id: 6580790444 and post id: 6580676411 at q=10 match at .253
not matches: post id: 6470137879 and post id: 6537495772 match at .98

post id: 6580657583 and post id: 6134864208 matched at 0.3236878, they're not exactly the same but from the same development and same key blurb

post id: 6294562979 and  post id: 6496923767 matched at 0.3727811, they're clearly different listings (both 1bdrms but different sqft), but from the same development. They also have different feature lists. Still, they have a key shared paragraph. post id: 6335337190 is from the same development and matched ....3767 at 0.4171076. I'd still consider it a dupe.

Looking at some listings from the Maverick's apartment, matching with post id: 6215244664, ranged from low .4s to .58. They all had the same list of features, but the .58 with post id: 6134807465 was written before the development opened. Though the feature list was the same, the rest of the text was completely distinct. The same was true for post id: 6319614634, which matched at around .55. post id: 6209272455 matched at just over .50, and it could be a duplicate, but would be fine if it were maintained. post id: 6272918342 matched with a .49 and was too close, I'd want to drop it for sure. This is pushing me towards a .5 threshold, though dropping all of these texts would be OK too.

Ok, now I'm looking at some of Kevin Falk's listings, matching with post id: 6218383684. Looking at the highest match in the .40s, post id: 6487945574 at .43, it's for a different unit, but uses Kevin's same spiel abou the neighborhood (even thought they're from different places!). Post id: 6380109657, matching at .52, though, seemed like only slightly less of a match. It's notable that noting I've read under .6 that was certainly not a duplicate. However, at .569, post id: 6185438087 though it still had Kevin's format, had more separate content. I'd be OK with it as a separate listing. This pushes me towards a .55 threshold.

Now some from 57 Apartments matching with post id: 6528181107. Again, all of th listings under .6 seem to pe actually from 57 apartments and are quite similar. Looking at post id: 6230206243 which matched with about a .55, it gets this high difference rating because it has some different featers. However, I would mark it as a duplicate. This is pushing me towards a higher threshold.



Now some from "the martin" matching with post id: 6375324984. Even post id: 6580914693, matching at .68, is from the martin, but it is significantly different. There's only one paragraph of boilerplate and the rest seems to be describing a different unit. This makes me want to consider a .6 threshold. I'm going to grap some more listings from above .7 even to try and find some false duplicates

I spent some real time trying to get up to a true false duplicate. post id: 6445924806 matches with post id: 6254034227 at.8 and is similar in some ways, but is clearly not a duplicate. So .8 is too high. That's good to know.

I'm going to try a .7 threshold and see how it goes.

```{r test if dopped docs were actually dupes}
# grab a sample of 100 documents
sample_dupes <- cl_dropped %>% sample_n(100)
# check each agianst the corpus and count matches
sample_dupes['matches'] = 0
for(i in 31:60){
  line = sample_dupes[i,]
  dist_matrix <- stringdistmatrix(line$cleanText, cl_data$cleanText, method = 'jaccard', q = 10)
  to.8 <- as.data.frame(which(as.matrix(dist_matrix)<.8&as.matrix(dist_matrix)>.7, arr.ind = T)) %>% 
    filter(row!=col) %>% 
    mutate(candidate = ifelse(row>col,row,col))
  to.7 <- as.data.frame(which(as.matrix(dist_matrix)<.7&as.matrix(dist_matrix)>.6, arr.ind = T)) %>% 
    filter(row!=col) %>% 
    mutate(candidate = ifelse(row>col,row,col))
  to.6 <- as.data.frame(which(as.matrix(dist_matrix)<.6&as.matrix(dist_matrix)>.5, arr.ind = T)) %>% 
    filter(row!=col) %>% 
    mutate(candidate = ifelse(row>col,row,col))
  sample_dupes$matches[i] <- length(unique(to.7$candidate)) + length(unique(to.6$candidate))
  print(paste("Matches for line",i))
  print(length(unique(to.8$candidate)))
  print(length(unique(to.7$candidate)))
  print(length(unique(to.6$candidate)))
  hist(dist_matrix)
}
# report average matches 

40,46,52,60
i=46
line = sample_dupes[i,]
dist_matrix <- stringdistmatrix(line$cleanText, cl_data$cleanText, method = 'jaccard', q = 10)
to.8 <- as.data.frame(which(as.matrix(dist_matrix)<.8&as.matrix(dist_matrix)>.7, arr.ind = T)) %>% 
  filter(row!=col) %>% 
  mutate(candidate = ifelse(row>col,row,col))
to.7 <- as.data.frame(which(as.matrix(dist_matrix)<.7&as.matrix(dist_matrix)>.6, arr.ind = T)) %>% 
  filter(row!=col) %>% 
  mutate(candidate = ifelse(row>col,row,col))
to.6 <- as.data.frame(which(as.matrix(dist_matrix)<.6&as.matrix(dist_matrix)>.5, arr.ind = T)) %>% 
  filter(row!=col) %>% 
  mutate(candidate = ifelse(row>col,row,col))
sample_dupes$matches[i] <- length(unique(candidates$candidate))
print(paste("Matches for line",i))
print(length(unique(to.8$candidate)))
print(length(unique(to.7$candidate)))
print(length(unique(to.6$candidate)))
match_type = c(rep.int(.8,length(unique(to.8$candidate))), rep.int(.7,length(unique(to.7$candidate))), rep.int(.6,length(unique(to.6$candidate))))
matches <- bind_rows(cl_data[to.8$candidate,], cl_data[to.7$candidate,], cl_data[to.6$candidate,]) %>% mutate(match_type = match_type)
dist_matrix_limited <- stringdistmatrix(line$cleanText, matches$cleanText, method = 'jaccard', q=10)
cat(line$cleanText)
cat(matches$cleanText[2])
matches$postID[84]
hist(dist_matrix[dist_matrix<.9])
test <- cl_dropped %>% filter(matchAddress=='1221 1st Ave')
stringdist(test$cleanText[[5]], test$cleanText[[7]], method = 'jaccard', q=10)
```


```{r train-test split}
cl_train <- sample_frac(cl_dropped, .5)
cl_test <- cl_dropped[!(cl_dropped$py_index %in% cl_train$py_index),]
```

Run STM Model
```{r fit the models}
# run the STM text processor for additional cleaning, including stemming, removing punctuation, numbers, and words smaller than 3 characters
# preprocess the documents, make a corpus

## PROCESS TRAINING DATA
temp <- textProcessor(documents = cl_train$fullText, meta=cl_train, onlycharacter = TRUE) 
out <- prepDocuments(temp$documents, temp$vocab, meta = temp$meta)


## ROUTINE FOR FINDING BEST K
# search_k <- searchK(out$documents,
#                      out$vocab, 
#                      K = c(24,50,100),
#                      emtol = 0.00002,
#                      prevalence = ~ black_proportion + 
#                        asian_proportion + 
#                        latinx_proportion + 
#                        pop_thousands + 
#                        pov_proportion +
#                        log_income + 
#                        log_price, 
#                      data=out$meta
#                      )



# on 7-24 I decided to add bedrooms and sqft to the model because I don't think it should treat large adn small expensive units the same
full_model_40 <- stm(out$documents, 
                     out$vocab, 
                     K = 40,
                     prevalence = ~ black_proportion + 
                       asian_proportion + 
                       latinx_proportion + 
                       pop_thousands + 
                       pov_proportion +
                       log_income + 
                       log_price +
                       log_sqft, 
                     data=out$meta,
                     seed = 24)


# MODELS WITH OTHER SPECIFICATIONS

partial_40 <- stm(out$documents, 
                     out$vocab, 
                     K = 40,
                     prevalence = ~ black_proportion + 
                       asian_proportion + 
                       latinx_proportion,
                     data=out$meta,
                     seed = 24)

empty_40 <- stm(out$documents, 
                     out$vocab, 
                     K = 40,
                     LDAbeta = FALSE,
                     data=out$meta,
                     seed = 24)

#full_model_50_spline <- stm(out$documents, out$vocab, K = 50,prevalence = ~ s(black_proportion) + s(asian_proportion) + s(latinx_proportion) + pop_thousands + pov_proportion + log_income + log_price, data=out$meta)


#full_model_100 <- stm(out$documents, out$vocab, K = 100,prevalence = ~ black_proportion + asian_proportion + latinx_proportion + pop_thousands + pov_proportion + log_income + log_price, data=out$meta)

#white_model_50 <- stm(out$documents, out$vocab, K = 50,prevalence = ~ white_proportion + asian_proportion + black_proportion + pop_thousands + pov_proportion + log_income + log_price, data=out$meta)

#white_model_100 <- stm(out$documents, out$vocab, K = 100, prevalence = ~ white_proportion + asian_proportion + latinx_proportion + pop_thousands + pov_proportion + log_income + log_price, data=out$meta)

#empty_model_100 <- stm(out$documents, out$vocab, K = 100, emtol = 0.0001, data=out$meta)

#empty_model_100_sage <- stm(out$documents, out$vocab, K = 100, emtol = 0.0001,LDAbeta = FALSE, data=out$meta)

```


It's hard to pick from those models, but it's clear that I want to include non-white racial groups and poverty, so I'm going to run a model selection tool
```{r model selection}
# store <- manyTopics(out$documents, 
#                      out$vocab, 
#                      K = c(24,50,75,100),
#                      prevalence = ~ black_proportion + 
#                        asian_proportion + 
#                        latinx_proportion + 
#                        pop_thousands + 
#                        pov_proportion +
#                        log_income + 
#                        log_price, 
#                      data=out$meta,
#                      runs = 24,
#                      init.type = "Spectral"
#                     )
```



```{r}
model <- full_model_50
model_fit <-  as_data_frame(make.dt(model, out$meta))
# labelTopics(model, n = 12)
```

Visualizations

```{r}
n_topics = 40
effects <- estimateEffect(1:n_topics ~ black_proportion + latinx_proportion + asian_proportion + pop_thousands + pov_proportion + log_income + log_price + log_sqft, model, out$meta)
sum == summary(effects)

```


```{r}
coef_sum <- data.frame()
for(i in 1:n_topics){
  row <- data.frame(topic = paste('Topic',i),
                    race = c('black','latinx','asian'),
                    coefs = c(sum$tables[i][[1]][2,1],sum$tables[i][[1]][3,1],sum$tables[i][[1]][4,1]), 
                    stderrs = c(sum$tables[i][[1]][2,2],sum$tables[i][[1]][3,2],sum$tables[i][[1]][4,2])
                    )
  coef_sum <- bind_rows(coef_sum, row)
}
coef_sum %>% mutate(high_est = coefs + stderrs, low_est = coefs - stderrs, topic = factor(topic)) %>%    
  #filter(race=='black') %>%
  filter(abs(coefs) > .05) %>% 
  ggplot(aes(x = reorder(topic, -coefs), y = coefs))+
    geom_hline(yintercept = 0, color = "red", alpha=.5) + # plot a line at 0
    geom_pointrange(aes(ymax = high_est, ymin = low_est, color=race), size=.2)+
    #geom_text(aes(y = coefs + (.0009*nchar(title)), label = title)) +
    #geom_text(aes(label = topic), nudge_y = -.009) +
    #theme_minimal() + # auto exlude backgound shading
    theme(text = element_text(size=10))+ #, # set values for text
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
    ggtitle("Estimated Effects of neighborhood racial proportions on Topic Distributions")+
    theme(text = element_text(size=10), #, # set values for text
          #axis.text.y = element_blank(),
          #axis.title.x = element_blank(),
          axis.title.y = element_blank()) +
    ylim(-max(abs(coef_sum$coefs))-.05,max(abs(coef_sum$coefs))+.05)+
    #ylim(-.12,.09)+
    coord_flip() # remove gridlines

#coef_sum %>% filter(race=='asian') %>% arrange(desc(abs(coefs))) #%>% select(topic)
```





Ok, what's the deal here, what do you want from this:
- I want texts that represent the topics
- I want texts that represent various kinds of neighborhoods
- I want to limit the total number of texts I have to deal with

```{r export key texts}
coef_sum <- data.frame()
for(i in 1:n_topics){
  row <- data.frame(topic = paste('Topic',i),
                    race = c('black','latinx','asian','none'),
                    coefs = c(sum$tables[i][[1]][2,1],sum$tables[i][[1]][3,1],sum$tables[i][[1]][4,1],0), 
                    stderrs = c(sum$tables[i][[1]][2,2],sum$tables[i][[1]][3,2],sum$tables[i][[1]][4,2],0)
                    )
  coef_sum <- bind_rows(coef_sum, row)
}


grab_top_texts <- function(df, topic, n, covariate='none', thresh=NULL){
  ordered <- df[order(-df[[topic]]),] # order the dataframe by the given topic 
  if(covariate!='none'){
    if(is.null(thresh)){ # if there's no threshold
      thresh = median(cl_train[[covariate]]) # use the median in the data
    }
    return(head(ordered[ordered[[covariate]]>thresh,], n))#return the top 'n' matches by py_index
  }
  return(head(ordered, n))
}

# prepares a df of the top n texts that match certain topics and covariates
prep_df <- function(topic,n=10, covariates = NULL){
  top_texts <- data.frame() # empty df
  if(is.null(covariates)){ # set defaults if necessary
    covariates <- c('none',"black_proportion", "latinx_proportion", "asian_proportion")
  }
  if(is.data.frame(covariates)){
    covariates = c('none', top_coef$race[top_coef$topic==paste0('Topic ',str_extract(topic,'\\d+'))])
    print(covariates)
  }
  for(covariate in covariates){ # loop through covariates
    top_list <- grab_top_texts(model_fit[!(model_fit$py_index %in% top_texts$py_index),],topic,n,covariate) # grab the top n that aren't already grabbed
    top_list['covariate'] <-  covariate # record the covariate name
    top_texts <- rbind(top_texts, top_list) # bind to the top texts
  }
  top_texts['topic'] = topic # record the topic
  return(top_texts)
}
# a function to produce text output
text_output <- function(py_index,df,filepath){
  df = df[df$py_index==py_index,]
  filepath = paste0(filepath,paste(df$topic, df$postID, sep='_'),'.txt') # include the topic and post id in the filename
  # prepare file contents
  contents = paste0("This text was a ",round(df[df$topic],3), " match for ", df$topic, 
                   " and assoicated with postID ", df$postID,
                   " at ", df$matchAddress, " in tract ", df$GEOID10,
                   ", and cost ", df$cleanRent,
                   "\nIt was associated with above median ", df$covariate, "\n\n",
                   'Title: ',df$listingTitle, '\n\nText: ',df$listingText
                   ) 
  # write the file
  write_file(contents, filepath)
}

```


```{r}
# make a df that helps extract useful texts
top_coef <- coef_sum %>% group_by(topic) %>% summarise(min = min(coefs), max = max(coefs)) %>% mutate(coefs = max) %>% inner_join(coef_sum) %>% mutate(race = as.character(race))
# grab the texts
# add to a dataframe with some metadata
top_texts <- map_dfr(paste0('Topic', 1:n_topics), prep_df, n=5, covariates=top_coef) # map through the topics and make a df
# walk the dataframe to make files
walk(top_texts$py_index, text_output, df=top_texts, filepath = 'output/text/')

#prep extra texts
extra <- model_fit %>% arrange(desc(topic_39)) %>% filter(!(py_index %in% top_texts$py_index)) %>% head(5) %>% mutate(topic = 'topic_39', covariate = 'none')
walk(extra$py_index, text_output, df=extra, filepath = 'output/text/extras/')
```

1(1) not a true dupe, 
3(2) one true dupe and one sep unit, 
6(1) not a dupe, same property mangement, 
7(0), 
9(0), 
12(0), 
13(0), 
15(1) true dupe, 
21(0), 
22(0), 
28(1) true dupe, 
29(1) different units, same building, 
30(0), 
31(0), 
32(0), 
36(0), 
38(0)
topic 29 doc 8
spacious unit with gorgeous lake views blue dolphin apartments is currently offering a spacious corner 1 bedroom unit with gorgeous peek a boo views of lake washington unit offers hardwood floors a gas fireplace dishwasher and microwave the blue dolphin building overlooks lake washington and has a beautiful lawn that rolls down to 120 of lakefront enjoy swimming bbqing or sunning on the dock we also offer boat moorage with slips available for rental we are located just one block south of s restaurants coffee shops and groceries and we are just a 7 minute drive to rent 1695 per month 720 security deposit no smoking no pets 400 lakeside ave s please call or email lori at show contact info bluedolphinapartments gmail 

```{r testing for dupe thresh}
top_10 <- top_texts %>% filter(topic == 'Topic38')
dist_matrix <- stringdistmatrix(top_10$cleanText, method = 'jaccard', q =10)
candidates <- as.data.frame(which(as.matrix(dist_matrix)<.7, arr.ind = T)) %>%
  filter(row!=col) %>%
  mutate(candidate = ifelse(row>col,row,col))
dist_matrix
cat(top_10$fullText[[8]])
```



```{r}
names(cl_full)
cl_dropped%>% group_by(matchAddress) %>% count() %>% arrange(desc(n))
```



```{r}
ggplot(test %>% group_by(listingDate) %>% summarise(conviction = mean(conviction), n = first(n)))+
  geom_point(aes(listingDate, conviction))+
  geom_smooth(aes(listingDate, conviction))+
  ggtitle("Prevalence of the word 'conviction' over time in CL ads")
test['conviction'] = str_detect(string = test$listingText,pattern = 'conviction')
test['criminal'] = str_detect(string = test$listingText,pattern = 'conviction')

test$criminal = test$criminal + test$background
test %>% group_by(listingDate) %>% summarise(criminal = mean(criminal), n = first(n)) %>% arrange(desc(criminal))
test %>% filter(listingDate == '2018-04-23', criminal==TRUE) %>% select(listingText)
```

```{r based on Chris Hess's work}

#### Helper script to compute map + point density map

#libraries


map_cl <- model_fit %>%
  filter(topic_40>.3)

bounds = c(min(map_cl$lng), min(map_cl$lat), max(map_cl$lng), max(map_cl$lat))
cl_map <- get_map(location=bounds, source="stamen", maptype = "toner", force = FALSE)

ggmap(cl_map)+
  geom_point(aes(lng,lat,color=topic_13), alpha = .5, data = map_cl)


```




############################                        ############################ 
############################   Expand to Test Set   ############################
############################                        ############################



```{r align the new documents with the old}
temp_test <- textProcessor(documents = cl_test$fullText, meta=cl_test, onlycharacter = TRUE) 
out_test <- alignCorpus(temp_test, model$vocab, verbose = TRUE)
new_fit_average <- fitNewDocuments(model=model, 
                documents=out_test$documents, 
                newData=out_test$meta,
                origData=out$meta, 
                prevalence=~ black_proportion + 
                       asian_proportion + 
                       latinx_proportion + 
                       pop_thousands + 
                       pov_proportion +
                       log_income + 
                       log_price +
                       log_sqft,
                prevalencePrior="Average",
                verbose = TRUE)
new_fit_covariate <- fitNewDocuments(model=model, 
                documents=out_test$documents, 
                newData=out_test$meta,
                origData=out$meta, 
                prevalence=~ black_proportion + 
                       asian_proportion + 
                       latinx_proportion + 
                       pop_thousands + 
                       pov_proportion +
                       log_income + 
                       log_price +
                       log_sqft,
                prevalencePrior="Covariate",
                verbose = TRUE)
```

```{r}
new_fit_topics = make.dt(new_fit_average) %>% inner_join(cl_test %>% mutate(docnum=as.numeric(rownames(cl_test))))
test_model <- glm(Topic18 ~ black_proportion + 
                       asian_proportion + 
                       latinx_proportion + 
                       pop_thousands + 
                       pov_proportion +
                       log_income + 
                       log_price +
                       log_sqft, data = new_fit_topics)
summary(test_model)
```



```{r}
model <- full_model_50
model_gamma <- tidy(model, matrix = "gamma") # get topic proprtions by document
model_fit <- spread(model_gamma,topic,gamma) %>% 
    rename_at(vars(`1`:`50`), 
              funs(paste0('topic_',.))) %>% # rename topics
    mutate(py_index = temp$meta$py_index) # spread the proportions
```

Visualizations

```{r}
n_topics = 50
model_new <- model
model_new$theta = new_fit_average$theta
model_new$eta = new_fit_average$eta
effects <- estimateEffect(1:n_topics ~ black_proportion + latinx_proportion + asian_proportion + pop_thousands + pov_proportion + log_income + log_price + log_sqft, model_new, out_test$meta)
sum = summary(effects)



```


```{r}
coef_sum <- data.frame()
for(i in 1:n_topics){
  row <- data.frame(topic = paste('Topic',i),
                    race = c('black','latinx','asian'),
                    coefs = c(sum$tables[i][[1]][2,1],sum$tables[i][[1]][3,1],sum$tables[i][[1]][4,1]), 
                    stderrs = c(sum$tables[i][[1]][2,2],sum$tables[i][[1]][3,2],sum$tables[i][[1]][4,2])
                    )
  coef_sum <- bind_rows(coef_sum, row)
}
coef_sum %>% mutate(high_est = coefs + stderrs, low_est = coefs - stderrs, topic = factor(topic)) %>%  filter(race=='black') %>%
  filter(abs(coefs) > .06) %>% 
  ggplot(aes(x = reorder(topic, -coefs), y = coefs))+
    geom_hline(yintercept = 0, color = "red", alpha=.5) + # plot a line at 0
    geom_pointrange(aes(ymax = high_est, ymin = low_est, color=race), size=.2)+
    #geom_text(aes(y = coefs + (.0009*nchar(title)), label = title)) +
    #geom_text(aes(label = topic), nudge_y = -.009) +
    #theme_minimal() + # auto exlude backgound shading
    theme(text = element_text(size=10))+ #, # set values for text
    theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())+
    ggtitle("Estimated Effects of neighborhood racial proportions on Topic Distributions")+
    theme(text = element_text(size=10), #, # set values for text
          #axis.text.y = element_blank(),
          #axis.title.x = element_blank(),
          axis.title.y = element_blank()) +
    ylim(-max(abs(coef_sum$coefs))-.05,max(abs(coef_sum$coefs))+.05)+
    coord_flip() # remove gridlines

#coef_sum %>% filter(race=='asian') %>% arrange(desc(abs(coefs))) #%>% select(topic)
```


ROBUSTNESSS: DOES IT WORK IF I ONLY USE NON_DUPES

There might be duplicates of my 12354 training set texts in the test set. What happens if I de-duplicate the test set with the training set and then test again?

```{r}
cl_test_dropped <- cl_test %>% bind_rows(cl_dropped %>% mutate(py_index = 999999))
```



```{r lame loopy dupe cleaning -- exploration}
# loopy loops
# init a high model total
cl_train_dropped <- cl_train
thresh = .8
interval = 49
model_total=51
n_topics = 40
i = 'Topic1'
# loop while the model total is high
while(model_total>40){
  model_total=0 # reset model total
  # preprocess the documents, make a corpus
  temp <- textProcessor(documents = cl_train_dropped$cleanText, meta=cl_train_dropped, onlycharacter = TRUE)
  out <- prepDocuments(temp$documents, temp$vocab, meta = temp$meta)
  # fit a limited model to that data
  dupe_model <- stm(out$documents, out$vocab, K = n_topics, max.em.its = 50,verbose = TRUE)
  dupe_fit <- make.dt(dupe_model,meta = cl_train_dropped %>% select(-(Topic1:Topic40),-docnum))
  # start some counters
  it_start = 1 
  it_total = 701
  # loop through iterations
  while(it_total>100){
    it_total = 0
    it_max = 0
    # loop through topics
    for(i in paste0('Topic',1:n_topics)){
      # grab the top 100 documents matching each topic
      top_100 <- dupe_fit %>% 
        arrange(get(i)) %>% 
        select(py_index) %>% 
        slice(it_start:(it_start+interval)) %>% 
        inner_join(cl_train_dropped, by='py_index') %>%
        select(py_index,cleanText)
      # get the texts of those documents and compare them
      dist_matrix <- stringdistmatrix(cl_train_dropped$cleanText, method = 'jaccard')
      candidates <- as.data.frame(which(as.matrix(dist_matrix)<thresh, arr.ind = T)) %>% 
        filter(row!=col) %>% 
        mutate(candidate = ifelse(row>col,col,row))
      # get the ids of the ones to drop
      drop_list <- top_100$py_index[c(unique(candidates$candidate))]
      # drop them
      cl_train_dropped <- cl_train_dropped %>% filter(!py_index %in% drop_list)
      # drop them from the fit object too
      dupe_fit <- dupe_fit %>% filter(!py_index %in% drop_list)
      print(paste("Dropped", length(drop_list), "listings from",i)) # print for logging
      # update the total for this iteration
      it_total <- it_total + length(drop_list)
      # if this topic beat the last max for this iteration update it
      if(length(drop_list)>it_max){
        it_max = length(drop_list)
      }
    }
    # update the start for this iteration
    it_start = it_start + 100 - it_max
    print(paste("Dropped",it_total,"total listings this iteration")) # print for logging
    model_total <- model_total + it_total
  }
  print(paste("Dropped",model_total,"total listings with this model")) # print for logging
}
```


```{r}
it_total = 0
it_max = 0
thresh = .7
dupe_fit <- model_fit
dupe_model <- model
cl_train_dropped <- cl_train
# loop through topics
addresses <- cl_train_dropped%>% group_by(matchAddress) %>% count() %>% arrange(desc(n)) %>% filter(n>10)
for(i in addresses$matchAddress){
  # grab the top 100 documents matching each topic
  top_100 <- cl_train_dropped %>% filter(matchAddress == i)
  # get the texts of those documents and compare them
  dist_matrix <- stringdistmatrix(top_100$cleanText, method = 'jaccard')
  candidates <- as.data.frame(which(as.matrix(dist_matrix)<thresh, arr.ind = T)) %>% 
    filter(row!=col) %>% 
    mutate(candidate = ifelse(row>col,col,row))
  # get the ids of the ones to drop
  drop_list <- top_100$py_index[c(unique(candidates$candidate))]
  # drop them
  cl_train_dropped <- cl_train_dropped %>% filter(!py_index %in% drop_list)
  # drop them from the fit object too
  dupe_fit <- dupe_fit %>% filter(!py_index %in% drop_list)
  print(paste("Dropped", length(drop_list), "listings from",i)) # print for logging
  # update the total for this iteration
  it_total <- it_total + length(drop_list)
  # if this topic beat the last max for this iteration update it
  if(length(drop_list)>it_max){
    it_max = length(drop_list)
  }
}
```

