{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from importlib import reload\n",
    "\n",
    "import preprocess as pre\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the CL data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/no_dupes_lda_fit5_18.csv')\n",
    "# This has the latest preproc texts\n",
    "df = pd.read_csv('data/no_dupes_lda_fit5_18.csv', index_col=0, dtype = {'GEOID10':object,'blockid':object, 'postid':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14748"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compare count and overlap\n",
    "black = df.index[df['high_black'] == True].tolist()\n",
    "white = df.index[df['high_white'] == True].tolist()\n",
    "asian = df.index[df['high_asian'] == True].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2196"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should I remove these from analysis?\n",
    "len(set(black).intersection(set(white)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "overlap = sorted(list(set(black).intersection(set(white))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(df.index[overlap])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12552"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3168"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(black).intersection(set(asian)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1218"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(white).intersection(set(asian)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6385"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(black)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6208"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(asian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "woah, asian & white neighbs have the lowest overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Are titles alone predictive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['listingTitle'], df['high_white'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.7982795647259258\n",
      "F1 score:  0.6679551604174719\n",
      "accuracy:  0.7262587635436584\n"
     ]
    }
   ],
   "source": [
    "word_vectorizer = CountVectorizer()\n",
    "word_vectorizer.fit(X_train)\n",
    "X_train_vectorized = word_vectorizer.transform(X_train)\n",
    "model = LogisticRegression(C=.1).fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict_proba(word_vectorizer.transform(X_test))[:,1]\n",
    "binary_pred = [0 if value <= 0.5 else 1 for value in predictions]\n",
    "\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "print('accuracy: ', accuracy_score(y_test, binary_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not really."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in neighborhood names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('resources/hoods.txt', 'r') as inf:\n",
    "    hoodnames = inf.read().splitlines()\n",
    "    #hoodnames = inf.read()\n",
    "    #hoodnames = re.split(r',\\s*', hoodnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curated list of Seattle-area neighborhoods -- some manually added in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighbs = \"\"\"\"'Adams' 'Alki' 'Arbor Heights' 'Atlantic' 'Ballard' 'Belltown' 'Bellevue' Bitter Lake'\n",
    " 'Bothell' 'Bremerton' 'Briarcliff' 'Brighton' 'Broadview' 'Broadway' 'Bryant' 'Capitol Hill' 'Cedar Park'\n",
    " 'Central Business District' 'Columbia City' 'Crown Hill' 'Dunlap'\n",
    " 'East Queen Anne' 'Eastlake' 'Everett' 'Fairmount Park' 'Fauntleroy' 'Federal Way' 'First Hill'\n",
    " 'Fremont' 'Gatewood' 'Genesee' 'Georgetown' 'Green Lake' 'Greenlake' 'Greenwood'\n",
    " 'Haller Lake' 'Harrison/Denny-Blaine' 'High Point' 'Highland Park'\n",
    " 'Holly Park' 'Industrial District' 'Interbay' 'International District' 'Issaquah' 'Kirkland'\n",
    " 'Laurelhurst' 'Lawton Park' 'Leschi' 'Lower Queen Anne' 'Loyal Heights'\n",
    " 'Madison Park' 'Madrona' 'Mann' 'Maple Leaf' 'Matthews Beach'\n",
    " 'Meadowbrook' 'Mid-Beacon Hill' 'Mill Creek' Minor' 'Montlake' 'Mount Baker' 'Newcastle'\n",
    " 'North Admiral' 'North Beach/Blue Ridge' 'North Beacon Hill'\n",
    " 'North College Park' 'North Delridge' 'North Queen Anne' 'Olympic Hills'\n",
    " 'Phinney Ridge' 'Pike-Market' 'Pinehurst' 'Pioneer Square' 'Portage Bay'\n",
    " 'Rainier Beach' 'Ravenna' 'Redmond' 'Renton' 'Riverview' 'Roosevelt' 'Roxhill' 'Seaview'\n",
    " 'Seward Park' 'Shoreline' 'South Beacon Hill' 'South Delridge' 'South Lake Union'\n",
    " 'South Park' 'Southeast Magnolia' 'Stevens' 'Sunset Hill'\n",
    " 'University District' 'U District' 'UDistrict' 'Victory Heights' 'View Ridge' 'Wallingford'\n",
    " 'Wedgwood' 'West Seattle' 'West Queen Anne' 'West Woodland' 'Westlake'\n",
    " 'Whittier Heights' 'Windermere' 'Yesler Terrace'\"\"\".split(\"'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hoods = [name.lower() for name in neighbs if re.match(r'\\w+', name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adams',\n",
       " 'alki',\n",
       " 'arbor heights',\n",
       " 'atlantic',\n",
       " 'ballard',\n",
       " 'belltown',\n",
       " 'bellevue',\n",
       " 'bothell',\n",
       " 'bremerton',\n",
       " 'briarcliff',\n",
       " 'brighton',\n",
       " 'broadview',\n",
       " 'broadway',\n",
       " 'bryant',\n",
       " 'capitol hill',\n",
       " 'cedar park',\n",
       " 'central business district',\n",
       " 'columbia city',\n",
       " 'crown hill',\n",
       " 'dunlap',\n",
       " 'east queen anne',\n",
       " 'eastlake',\n",
       " 'everett',\n",
       " 'fairmount park',\n",
       " 'fauntleroy',\n",
       " 'federal way',\n",
       " 'first hill',\n",
       " 'fremont',\n",
       " 'gatewood',\n",
       " 'genesee',\n",
       " 'georgetown',\n",
       " 'green lake',\n",
       " 'greenlake',\n",
       " 'greenwood',\n",
       " 'haller lake',\n",
       " 'harrison/denny-blaine',\n",
       " 'high point',\n",
       " 'highland park',\n",
       " 'holly park',\n",
       " 'industrial district',\n",
       " 'interbay',\n",
       " 'international district',\n",
       " 'issaquah',\n",
       " 'kirkland',\n",
       " 'laurelhurst',\n",
       " 'lawton park',\n",
       " 'leschi',\n",
       " 'lower queen anne',\n",
       " 'loyal heights',\n",
       " 'madison park',\n",
       " 'madrona',\n",
       " 'mann',\n",
       " 'maple leaf',\n",
       " 'matthews beach',\n",
       " 'meadowbrook',\n",
       " 'mid-beacon hill',\n",
       " 'mill creek',\n",
       " 'montlake',\n",
       " 'mount baker',\n",
       " 'newcastle',\n",
       " 'north admiral',\n",
       " 'north beach/blue ridge',\n",
       " 'north beacon hill',\n",
       " 'north college park',\n",
       " 'north delridge',\n",
       " 'north queen anne',\n",
       " 'olympic hills',\n",
       " 'phinney ridge',\n",
       " 'pike-market',\n",
       " 'pinehurst',\n",
       " 'pioneer square',\n",
       " 'portage bay',\n",
       " 'rainier beach',\n",
       " 'ravenna',\n",
       " 'redmond',\n",
       " 'renton',\n",
       " 'riverview',\n",
       " 'roosevelt',\n",
       " 'roxhill',\n",
       " 'seaview',\n",
       " 'seward park',\n",
       " 'shoreline',\n",
       " 'south beacon hill',\n",
       " 'south delridge',\n",
       " 'south lake union',\n",
       " 'south park',\n",
       " 'southeast magnolia',\n",
       " 'stevens',\n",
       " 'sunset hill',\n",
       " 'university district',\n",
       " 'u district',\n",
       " 'udistrict',\n",
       " 'victory heights',\n",
       " 'view ridge',\n",
       " 'wallingford',\n",
       " 'wedgwood',\n",
       " 'west seattle',\n",
       " 'west queen anne',\n",
       " 'west woodland',\n",
       " 'westlake',\n",
       " 'whittier heights',\n",
       " 'windermere',\n",
       " 'yesler terrace']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data:\n",
    "- Strip URLs (or.. should map them to '#url' ???)\n",
    "- Map neighborhood names to '#hood'\n",
    "- Tokenize words & punctuation\n",
    "\n",
    "\n",
    "- don't use 'clean_text' yet since it has some preproc errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#url_pattern = r'(https?:\\/\\/)?(www)?.*[\\r\\n]*'\n",
    "\n",
    "punctuation_pattern = r\"[#\\w'-]+|[.,!?;]+\"\n",
    "url_pattern = r'(http)?(www)?\\S*(\\.com|\\.net|\\.gov|\\.be|\\.org)\\S*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "punctuation_pattern = r\"[#\\w'-]+|[.,!?;]+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    no_urls = re.sub(url_pattern, '', text)\n",
    "    for hood in hoodnames:\n",
    "        # hood_pattern = r'\\s+{0}\\s+'.format(hood)\n",
    "        #hood_pattern = r' ?'+hood+' ?'\n",
    "        # Match neighborhood mentions surrounded by whitespace and replace with #hood\n",
    "        no_urls = re.sub(r'\\W+{0}\\W+'.format(hood), ' #hood ', no_urls)\n",
    "    no_digits = re.sub(r'\\d+', '', no_urls)\n",
    "    tokenized = re.findall(punctuation_pattern, no_digits)\n",
    "    return ' '.join([word.lower() for word in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "short_example = \"this queen anne apartment is really cool !!! 98105 https://blah.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'QR Code Link to This Post\\r\\nContact info:\\r\\nLindsay |\\r\\nshow contact info\\r\\nMINUTES WALK TO GREENLAKE! BEAUTIFULLY REMODELED & NOW AVAILABLE!\\r\\n6515 5th Ave NE #102, Seattle, WA 98115\\r\\n$1,350/mo\\r\\nKEY FEATURES\\r\\nSq Footage:\\r\\n700 sqft.\\r\\nBedrooms:\\r\\n1 Bed\\r\\nBathrooms:\\r\\n1 Bath\\r\\nParking:\\r\\n2 Off street\\r\\nLease Duration:\\r\\n6 Months (See Details Below)\\r\\nDeposit:\\r\\n$600\\r\\nPets Policy:\\r\\nNo Pets Allowed\\r\\nLaundry:\\r\\nShared\\r\\nProperty Type:\\r\\nApartment\\r\\nDESCRIPTION\\r\\nWelcome to Greenlake Park Apartments; a smaller 7 unit building located just minutes walk to Greenlake! This spacious one bedroom apartment has just had a complete remodel with hardwood flooring, stainless appliances, granite, lighting, the works! The apartment home is available for immediate occupancy!\\r\\nWe have off street parking and additional storage. Laundry facilities are conveniently located on-site. Enjoy a flexible 6-12 month lease term.\\r\\nEnjoy living in Greenlake where you can walk to nearby shopping such as Whole Foods Market on Roosevelt and you are close to Safeway and Trader Joes. Lots of nearby dining, coffee houses, parks and transit is very convenient.\\r\\nVery easy online application can be found at:\\r\\non-site[dot]com\\r\\nrental application for 6515 5th Ave NE; Greenlake Park; $40 app fee\\r\\nBe the first to live in this beautifully remodeled one bedroom apartment home. Neighbors are fantastic and location is amazing!\\r\\nRENTAL FEATURES\\r\\nLiving room\\r\\nStorage space\\r\\nRange / Oven\\r\\nRefrigerator\\r\\nDishwasher\\r\\nMicrowave\\r\\nStainless steel appliances\\r\\nDouble pane / Storm windows\\r\\nCable-ready\\r\\nHardwood floor\\r\\nGranite countertop\\r\\nCOMMUNITY FEATURES\\r\\nNear transportation\\r\\nOff-street parking\\r\\nLEASE TERMS\\r\\nFlexible lease terms 6-12 okay\\r\\nWe have off-street parking!\\r\\nThis apartment is for immediate occupancy! Start your lease anytime from now to March 1!\\r\\nContact info:\\r\\nLindsay\\r\\nshow contact info'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_example = df.loc[100]['listingText']\n",
    "long_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this #hood apartment is really cool !!!'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(short_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    this queen anne apartment is really cool !!!_ URL\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.cl_clean_text(pd.Series(short_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qr code link to this post property description enjoy this inviting #hood rambler just minutes from #hood square mall , lincoln square , and all the amenities #hood hood has to offer . this home features hardwood floors , vaulted ceilings , and plenty of natural light . a spacious partially finished daylight basement features an additional living room , craft kitchen , and walk out to a full fenced yard . located in the acclaimed #hood school #hood and just a short walk to the #hood public library , this is the ideal home for everyone . pets are considered on a case by case basis . please contact adrian villanueva at show contact info or show contact info for more information . details availability date february , pets cats , small dogs lb pet notes pets determined on a case by case basis deposit . application fee amenities laundry in unit parking type covered parking notes attached carport appliances dishwasher , refrigerator , microwave , range oven exterior yard-fenced , porch heating cooling heat forced air , double pane storm windows wiring cable-ready , wired facilities and recreation minutes from king couty library and the #hood collection security and access security system views city additional features hardwood floor , fireplace , vaulted ceiling offered by north #hood properties equal housing opportunity to schedule a tour we now book our tour appointments online ! please see our available tour times and book your appointment online today or anytime or text yvqe to to schedule from your phone !'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(long_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Erica Property Management show contact info show contact info N th St Apt Seattle WA Cozy bedroom bathroom washerdryer in unit storage on balcony lots of space !___ BR BA Apartment $___ month Bedrooms Bathrooms full partial Sq Footage Parking Garage $___ Street Parking Pet Policy No pets Deposit $___ Non Refundable Administrative Fee $___ Application Fee $___ per person over years of age DESCRIPTION Gorgeous story newly remodeled complex equipped with elevators and garage parking Conveniently located near shopping centers schools major bus lines and easy I access see additional photos below RENTAL FEATURES Living room OfficeDen Dishwasher Refrigerator StoveOven Washer Dryer Balcony Deck or Patio Cableready Highspeed internet COMMUNITY FEATURES Garage parking Storage spaces Secured entry Elevator New property years LEASE TERMS MONTHS Contact info Erica Property Management show contact info show contact info Posted Nov pm PST'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.cl_clean_text(pd.Series(long_example)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [[word for word in text.lower().split() if word not in hoodnames] for text in df.clean_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply preproc to all texts\n",
    "df['preproc_text'] = df['body_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'preprocess' from '/Users/ikennedy/OneDrive - UW/UW/GIT/cl_lda/preprocess.py'>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.Series('6th streeet is the bestest klsjdlfkj ?? ~@!! jurekj.sdlfjs')\n",
    "text2 = df.listingText[df.postid== '6567473317.0'].values\n",
    "test = df.iloc[200:1200].copy()\n",
    "reload(pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['streeet bestest klsjdlfkj ?? !! jurekj sdlfjs'], dtype=object)"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('resources/seattle_stop_words.txt') as f:\n",
    "    neighborhoods = f.read().splitlines()\n",
    "from sklearn.feature_extraction import stop_words\n",
    "stop_words = neighborhoods + list(stop_words.ENGLISH_STOP_WORDS)\n",
    "stopword_pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*', flags=re.IGNORECASE)\n",
    "punctuation_pattern = r\"[#\\w']+|[!?]+\"\n",
    "url_pattern = r'(http)?(www)?\\S*(\\.com|\\.net|\\.gov|\\.be|\\.org)\\S*'\n",
    "short_pattern = r'^\\b\\w{1,3}\\b | \\b\\w{1,3}\\b'\n",
    "text = (text.str.lower() # make lowercase\n",
    "       .str.replace(stopword_pattern, '') # drop neighborhoods and other stopwords\n",
    "       .str.replace(url_pattern, '') # drop urls\n",
    "       .str.replace(r'\\d+', '') # drop digits\n",
    "       .str.findall(punctuation_pattern) # drop most punctuation\n",
    "       .str.join(' ') # join after punctuation drop\n",
    "       .str.replace(short_pattern, '')) # drop words with less than 3 characters\n",
    "text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.47 s ± 108 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pre.preprocess(test.listingText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.7 s ± 150 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit test['preproc_text'] = test['listingText'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save her for faster loading!\n",
    "df['preproc_text'] = pre.preprocess(df.listingText)\n",
    "df.to_csv('5_25_preproc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & test models.\n",
    "## Split the data into train & test sets\n",
    "## First, binary classif: high white vs not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['preproc_text'], df['high_white'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_vectorized = word_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Logistic regression model\n",
    "model = LogisticRegression(C=.1).fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(word_vectorizer.transform(X_test))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_pred = [0 if value <= 0.5 else 1 for value in predictions] #just use model.predict for these.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8585746093380979\n",
      "F1 score:  0.7699530516431925\n",
      "accuracy:  0.781389420012747\n"
     ]
    }
   ],
   "source": [
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "print('accuracy: ', accuracy_score(y_test, binary_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % (roc_auc))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(fpr, thresholds, markeredgecolor='r',linestyle='dashed', color='r')\n",
    "ax2.set_ylabel('Threshold',color='r')\n",
    "ax2.set_ylim([thresholds[-1],thresholds[0]])\n",
    "ax2.set_xlim([fpr[0],fpr[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['rail' 'westwood' 'concierge' 'marymoor' 'mall' 'airport' 'rianna' 'moda'\n",
      " 'aurora' 'harbor' 'riverpark' 'somerset' 'pools' 'jefferson' 'cedar'\n",
      " 'stadiums' 'fountain' 'cleveland' 'borgata' 'grand']\n",
      "\n",
      "Largest Coefs: \n",
      "['country' 'zoo' 'boutiques' 'domaine' 'village' 'esxpt' 'children'\n",
      " 'inglenook' 'bernard' 'woodland' 'volunteer' 'northshore' 'point' 'odin'\n",
      " 'springline' 'waterscape' 'beach' 'locks' 'urbana' 'nw']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(word_vectorizer.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "# Find the 10 smallest and 10 largest coefficients\n",
    "# The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "# so the list returned is in order of largest to smallest\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:841: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-99bf14f60714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mword_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mX_train_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m         \"\"\"\n\u001b[0;32m--> 836\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    837\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# Five-fold cross validation\n",
    "kf = KFold(n_splits=5)\n",
    "X, y = df['preproc_text'], df['high_white']\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    word_vectorizer.fit(X_train)\n",
    "    X_train_vectorized = word_vectorizer.fit_transform(X_train)\n",
    "    model = LogisticRegression(C=.1).fit(X_train_vectorized, y_train)\n",
    "    predictions = model.predict_proba(word_vectorizer.transform(X_test))[:,1]\n",
    "    binary_pred = [0 if value <= 0.5 else 1 for value in predictions]\n",
    "    print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "    print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "    print('accuracy: ', accuracy_score(y_test, binary_pred))\n",
    "    feature_names = np.array(word_vectorizer.get_feature_names())\n",
    "\n",
    "    # Sort the coefficients from the model\n",
    "    sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "    # Find the 10 smallest and 10 largest coefficients\n",
    "    # The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "    # so the list returned is in order of largest to smallest\n",
    "    print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "    print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ngram features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remake train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['preproc_text'], df['high_white'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngram_vectorizer = CountVectorizer(ngram_range=(1,4)).fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ngrams = ngram_vectorizer.transform(X_train)\n",
    "# Logistic regression model\n",
    "#model = LogisticRegression(C=.5).fit(X_train_ngrams, y_train)\n",
    "model = LogisticRegression(C=.5, penalty='l2').fit(X_train_ngrams, y_train)\n",
    "predictions = model.predict_proba(ngram_vectorizer.transform(X_test))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_pred = [0 if value <= 0.5 else 1 for value in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8898263399125023\n",
      "F1 score:  0.798804780876494\n",
      "accuracy:  0.8068833652007649\n"
     ]
    }
   ],
   "source": [
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "print('accuracy: ', accuracy_score(y_test, binary_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['mall' 'rail' 'airport' 'light rail' 'concierge' 'south'\n",
      " 'hood university' 'gym' 'shopping' 'center' 'college' 'marymoor'\n",
      " 'th ave hood' 'story' 'south hood' 'section' 'near' 'marymoor park'\n",
      " 'to shopping' 'westwood']\n",
      "\n",
      "Largest Coefs: \n",
      "['uw' 'ave nw' 'children' 'locks' 'hood beach' 'west' 'charming'\n",
      " 'hood ave' 'basement' 'point' 'smoking' 'laundry' 'village' 'on hood'\n",
      " 'deck' 'hood village' 'market' 'shops' 'beach' 'nw']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(ngram_vectorizer.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:841: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-e905d170475b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mX_train_vectorized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    793\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 266\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, doc)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[1;32m    120\u001b[0m                              \"unicode string.\")\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "# CV with ngram on high white\n",
    "kf = KFold(n_splits=5)\n",
    "X, y = df['preproc_text'], df['high_white']\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train_vectorized = ngram_vectorizer.fit_transform(X_train)\n",
    "    model = LogisticRegression(C=.5).fit(X_train_vectorized, y_train)\n",
    "    predictions = model.predict_proba(ngram_vectorizer.transform(X_test))[:,1]\n",
    "    binary_pred = [0 if value <= 0.5 else 1 for value in predictions]\n",
    "    print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "    print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "    print('accuracy: ', accuracy_score(y_test, binary_pred))\n",
    "    feature_names = np.array(ngram_vectorizer.get_feature_names())\n",
    "\n",
    "    # Sort the coefficients from the model\n",
    "    sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "    # Find the 10 smallest and 10 largest coefficients\n",
    "    # The 10 largest coefficients are being indexed using [:-11:-1]\n",
    "    # so the list returned is in order of largest to smallest\n",
    "    print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "    print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AP note (04/14) The neighborhood names are still not complete enough -_-\n",
    "# also they are still sneaking in somehow... need more work on preproc\n",
    "# 'kirkland' and 'issaquah' are IN the damn hoods list.. why are they still showing up?!\n",
    "# 5/22: updated regex in preproc, does much better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OK, that was a binary prediction on high-white; let's see high-black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prob a faster way to just grab new labels...\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['preproc_text'], df['high_black'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_ngrams = ngram_vectorizer.transform(X_train)\n",
    "# Logistic regression model\n",
    "model = LogisticRegression(C=.5).fit(X_train_ngrams, y_train)\n",
    "predictions = model.predict_proba(ngram_vectorizer.transform(X_test))[:,1]\n",
    "\n",
    "\n",
    "#Parameters to try: l1 penalty instead of l2\n",
    "# different regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_pred = [0 if value <= 0.5 else 1 for value in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.9070896497141245\n",
      "F1 score:  0.7472647702407001\n",
      "accuracy:  0.8527724665391969\n"
     ]
    }
   ],
   "source": [
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "print('accuracy: ', accuracy_score(y_test, binary_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['nw' 'ne' 'beach' 'uw' 'trails' 'trail' 'school hood' 'hood square'\n",
      " 'newport' 'island' 'courtyard' 'hood trail' 'blocks' 'hood beach' 'old'\n",
      " 'shops' 'hood park' 'car garage' 'ride' 'hardwood floors']\n",
      "\n",
      "Largest Coefs: \n",
      "['gated' 'college' 'freeway' 'hood station' 'aurora' 'golf' 'ave hood'\n",
      " 'north hood' 'rianna' 'station' 'th ave hood' 'mall' 'court' 'south'\n",
      " 'concierge' 'light rail' 'south hood' 'airport' 'hood university' 'rail']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(ngram_vectorizer.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8560934278030323\n",
      "F1 score:  0.6311522872032427\n",
      "accuracy:  0.7970044614404079\n"
     ]
    }
   ],
   "source": [
    "X_train_vectorized = word_vectorizer.transform(X_train)\n",
    "model = LogisticRegression(C=.1, penalty='l1').fit(X_train_vectorized, y_train)\n",
    "predictions = model.predict_proba(word_vectorizer.transform(X_test))[:,1]\n",
    "binary_pred = [0 if value <= 0.5 else 1 for value in predictions]\n",
    "print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "print('accuracy: ', accuracy_score(y_test, binary_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['nw' 'newport' 'ne' 'trader' 'winning' 'trails' 'gilman' 'volunteer'\n",
      " 'beach' 'se' 'comprehensive' 'marymoor' 'trail' 'lincoln' 'surrey' 'coin'\n",
      " 'springline' 'urbana' 'september' 'uw']\n",
      "\n",
      "Largest Coefs: \n",
      "['mall' 'cityline' 'swedish' 'hills' 'stadiums' 'cedar' 'aurora' 'olympus'\n",
      " 'helios' 'centennial' 'southcenter' 'sculpture' 'jefferson' 'concierge'\n",
      " 'rail' 'airport' 'westwood' 'rianna' 'harbor' 'moda']\n"
     ]
    }
   ],
   "source": [
    "feature_names = np.array(word_vectorizer.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1627/2060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try SVM..\n",
    "\n",
    "model = SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, random_state=42, max_iter=5, tol=None)\n",
    "model.fit(X_train_vectorized,y_train)\n",
    "predicted = model.predict(word_vectorizer.transform(X_test))\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.score(word_vectorizer.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Five-fold cross validation on unigrams\n",
    "kf = KFold(n_splits=5)\n",
    "X, y = df['preproc_text'], df['high_black']\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    word_vectorizer.fit(X_train)\n",
    "    X_train_vectorized = word_vectorizer.fit_transform(X_train)\n",
    "    model = LogisticRegression(C=.5).fit(X_train_vectorized, y_train)\n",
    "    predictions = model.predict_proba(word_vectorizer.transform(X_test))[:,1]\n",
    "    binary_pred = [0 if value <= 0.5 else 1 for value in predictions]\n",
    "    print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "    print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "    print('accuracy: ', accuracy_score(y_test, binary_pred))\n",
    "    feature_names = np.array(word_vectorizer.get_feature_names())\n",
    "\n",
    "    # Sort the coefficients from the model\n",
    "    sorted_coef_index = model.coef_[0].argsort()\n",
    "    print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "    print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:  0.8570017743120733\n",
      "F1 score:  0.7233669443226654\n",
      "accuracy:  0.7861016949152543\n",
      "Smallest Coefs:\n",
      "['nw' 'ne' 'hood square' 'uw' 'trails' 'newport' 'se' 'school hood'\n",
      " 'mountains' 'beach' 'shops' 'hood park' 'trail' 'ave nw' 'courtyard'\n",
      " 'square' 'ne hood wa' 'tour today' 'zoo' 'excellent']\n",
      "\n",
      "Largest Coefs: \n",
      "['flooring' 'elliott' 'mall' 'moda' 'olive' 'hood station' 'ave hood'\n",
      " 'harbor' 'pointe' 'th ave hood wa' 'south hood' 'golf' 'station'\n",
      " 'th ave hood' 'concierge' 'court' 'light rail' 'airport'\n",
      " 'hood university' 'rail']\n",
      "AUC:  0.860598941466387\n",
      "F1 score:  0.7143507972665147\n",
      "accuracy:  0.7874576271186441\n",
      "Smallest Coefs:\n",
      "['nw' 'ne' 'uw' 'hood square' 'trails' 'school hood' 'hood park' 'se'\n",
      " 'shops' 'beach' 'island' 'newport' 'square' 'ave nw' 'mountains' 'nw th'\n",
      " 'zoo' 'starting' 'ne hood' 'trail']\n",
      "\n",
      "Largest Coefs: \n",
      "['hood airport' 'south' 'golf course' 'the hood hood' 'community college'\n",
      " 'valley' 'ave hood' 'olive' 'concierge' 'court' 'south hood'\n",
      " 'hood station' 'station' 'pointe' 'th ave hood wa' 'light rail'\n",
      " 'th ave hood' 'hood university' 'airport' 'rail']\n",
      "AUC:  0.8571538507740589\n",
      "F1 score:  0.7303609341825903\n",
      "accuracy:  0.7847457627118644\n",
      "Smallest Coefs:\n",
      "['nw' 'ne' 'uw' 'hood square' 'school hood' 'mountains' 'shops' 'trails'\n",
      " 'hood park' 'newport' 'beach' 'in hood hood' 'square' 'se' 'island'\n",
      " 'hood trail' 'nw th' 'courtyard' 'ave nw' 'excellent']\n",
      "\n",
      "Largest Coefs: \n",
      "['elliott' 'ave hood' 'flooring' 'with large' 'hood station' 'rainier'\n",
      " 'golf' 'south' 'station' 'valley' 'pointe' 'harbor' 'concierge'\n",
      " 'south hood' 'th ave hood wa' 'th ave hood' 'airport' 'hood university'\n",
      " 'light rail' 'rail']\n",
      "AUC:  0.8859655566476079\n",
      "F1 score:  0.7694753577106518\n",
      "accuracy:  0.8033231603933537\n",
      "Smallest Coefs:\n",
      "['nw' 'ne' 'uw' 'hood square' 'square' 'se' 'trails' 'school hood'\n",
      " 'newport' 'island' 'shops' 'beach' 'nw th' 'hood park' 'mountains'\n",
      " 'se hood wa' 'ave nw' 'campus' 'se hood' 'trail']\n",
      "\n",
      "Largest Coefs: \n",
      "['apartment homes' 'moda' 'hood station' 'rianna' 'pointe' 'aurora' 'line'\n",
      " 'concierge' 'station' 'gated' 'harbor' 'valley' 'south hood' 'golf'\n",
      " 'light rail' 'th ave hood wa' 'th ave hood' 'hood university' 'airport'\n",
      " 'rail']\n",
      "AUC:  0.8878456245077312\n",
      "F1 score:  0.7928519328956966\n",
      "accuracy:  0.8073923363852153\n",
      "Smallest Coefs:\n",
      "['nw' 'ne' 'uw' 'hood square' 'trails' 'island' 'square' 'school hood'\n",
      " 'shops' 'se' 'blocks' 'newport' 'off the' 'town center' 'beach' 'ave nw'\n",
      " 'walking' 'hood town center' 'hood park' 'nw th']\n",
      "\n",
      "Largest Coefs: \n",
      "['roof' 'court' 'moda' 'westwood' 'hood station' 'olive' 'west hood'\n",
      " 'golf' 'harbor' 'ave hood' 'city' 'th ave hood wa' 'concierge'\n",
      " 'th ave hood' 'south hood' 'light rail' 'station' 'hood university'\n",
      " 'airport' 'rail']\n"
     ]
    }
   ],
   "source": [
    "# CV with ngram\n",
    "kf = KFold(n_splits=5)\n",
    "X, y = df['preproc_text'], df['high_black']\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    X_train_vectorized = ngram_vectorizer.fit_transform(X_train)\n",
    "    model = LogisticRegression(C=.5).fit(X_train_vectorized, y_train)\n",
    "    predictions = model.predict_proba(ngram_vectorizer.transform(X_test))[:,1]\n",
    "    binary_pred = [0 if value <= 0.5 else 1 for value in predictions]\n",
    "    print('AUC: ', roc_auc_score(y_test, predictions))\n",
    "    print('F1 score: ', f1_score(y_test, binary_pred))\n",
    "    print('accuracy: ', accuracy_score(y_test, binary_pred))\n",
    "    feature_names = np.array(ngram_vectorizer.get_feature_names())\n",
    "\n",
    "    # Sort the coefficients from the model\n",
    "    sorted_coef_index = model.coef_[0].argsort()\n",
    "    print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:20]]))\n",
    "    print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[-20:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
